{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/haosulab/SAPIEN-tutorial/blob/master/robotics/1_getting_started_with_robot.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Some core features of SAPIEN are not available on Colab, including the interactive viewer and ray-tracing functionalities. You need to run SAPIEN locally for full features. You can also find the latest SAPIEN tutorial at [SAPIEN's documentation](https://sapien.ucsd.edu/docs/latest/index.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robotics in SAPIEN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial series, you will learn how to use SAPIEN to work with robots and tackle basic robotics tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robotics Tutorial 1: Getting Started with Robot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you will learn the following:\n",
    "\n",
    "- Load a robot (URDF)\n",
    "- Set joint positions\n",
    "- Compensate passive forces\n",
    "- Control the robot by torques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-IXA-7qRy4ML"
   },
   "source": [
    "## Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ssNPcLKYDoDH"
   },
   "source": [
    "> Note: you need GPU runtime to run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qIDL-wHKzKQ5"
   },
   "outputs": [],
   "source": [
    "%pip install sapien\n",
    "\n",
    "import sapien.core as sapien\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "balance_passive_force = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the engine, renderer and scene"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's set up the simulation environment. You might have been very familiar with this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sapien.Engine()\n",
    "engine.set_log_level(\"critical\")\n",
    "renderer = sapien.SapienRenderer()\n",
    "engine.set_renderer(renderer)\n",
    "\n",
    "scene_config = sapien.SceneConfig()\n",
    "scene = engine.create_scene(scene_config)\n",
    "scene.set_timestep(1 / 240.0)\n",
    "scene.add_ground(0)\n",
    "\n",
    "# Light and camera are for later visualization\n",
    "scene.set_ambient_light([0.5, 0.5, 0.5])\n",
    "scene.add_directional_light([0, 1, -1], [0.5, 0.5, 0.5])\n",
    "camera = scene.add_camera(name='camera', width=1024, height=768, fovy=1.57, near=0.1, far=100)\n",
    "camera.set_pose(sapien.Pose([-0.9, 0, 0.6], [0.988771, 0, 0.149438, 0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a robot URDF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can create a `URDFLoader` to load the URDF XML of Kinova Jaco2 arm. [URDF XML](http://wiki.ros.org/urdf/XML) describes a robot. Usually, URDF files are provided by manufacturers. For example, the URDF XML of Kinova Jaco2 arm can be found [here](https://github.com/Kinovarobotics/kinova-ros)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !gdown -q 10-ASSF6hiMLoMfajrTBMNn_U3YyP6-sl\n",
    "    !unzip -o -q jaco2.zip\n",
    "    assets_dir = \".\"\n",
    "else:\n",
    "    assets_dir = \"../assets\"\n",
    "\n",
    "loader: sapien.URDFLoader = scene.create_urdf_loader()\n",
    "print(\"fix_root_link: \", loader.fix_root_link)\n",
    "robot: sapien.Articulation = loader.load(os.path.join(assets_dir, \"jaco2/jaco2.urdf\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is a `fix_root_link` flag for the URDF loader. If it is true (by default), then the root link of the robot will be fixed. Otherwise, it is allowed to move freely.\n",
    "\n",
    "> Note: When a robot is already loaded, changing the flag of the URDF loader will not take effect.\n",
    "\n",
    "The robot is loaded as `Articulation`, which is a tree of links connected by joints. We can set the pose of its root link through `set_root_pose(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set root link pose\n",
    "robot.set_root_pose(sapien.Pose([0, 0, 0], [1, 0, 0, 0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set joint positions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also set initial joint positions through `set_qpos(qpos=...)`. The `qpos` should be a concatenation of the position of each joint. Its length is the degree of freedom, and its order is the same as that returned by `robot.get_joints()`.\n",
    "\n",
    "> Note: If the articulation is loaded from a URDF file, its joints are in preorder (DFS preorder traversal over the articulation tree). If the articulation is built programmatically (as introduced in basics: create articulations), its joints are in the order when they are built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial joint positions\n",
    "arm_init_qpos = [4.71, 2.84, 0, 0.75, 4.62, 4.48, 4.88]\n",
    "gripper_init_qpos = [0, 0, 0, 0, 0, 0]\n",
    "init_qpos = arm_init_qpos + gripper_init_qpos\n",
    "robot.set_qpos(init_qpos)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what will happen if we run the simulation loop. Run the following cell and you will see 10 screenshots of the robot during the simulation. If you are running locally, an additional viewer window will be shown to display the animation of the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "print(\"robot initial qpos: \", robot.get_qpos())\n",
    "\n",
    "if IN_COLAB:\n",
    "    for steps in range(200):\n",
    "        if balance_passive_force: # This will be explained in next section\n",
    "            qf = robot.compute_passive_force(gravity=True, coriolis_and_centrifugal=True)\n",
    "            robot.set_qf(qf)\n",
    "\n",
    "        scene.step()\n",
    "\n",
    "        if steps % 20 == 0:\n",
    "            scene.update_render()\n",
    "            camera.take_picture()\n",
    "            rgba = camera.get_color_rgba()\n",
    "            plt.subplot(2, 5, steps//20+1)\n",
    "            plt.imshow(rgba[..., :3])\n",
    "    \n",
    "    print(\"robot qpos at steps 199: \", robot.get_qpos())\n",
    "else:\n",
    "    from sapien.utils import Viewer\n",
    "    viewer = Viewer(renderer)\n",
    "    viewer.set_scene(scene)\n",
    "    viewer.set_camera_xyz(x=-0.8, y=0, z=0.4)\n",
    "    viewer.set_camera_rpy(r=0, p=-0.3, y=0)\n",
    "\n",
    "    steps = 0\n",
    "    while not viewer.closed:\n",
    "        if balance_passive_force: # This will be explained in next section\n",
    "            qf = robot.compute_passive_force(gravity=True, coriolis_and_centrifugal=True)\n",
    "            robot.set_qf(qf)\n",
    "\n",
    "        scene.step()\n",
    "        scene.update_render()\n",
    "        viewer.render()\n",
    "\n",
    "        if steps < 200 and steps % 20 == 0:\n",
    "            scene.update_render()\n",
    "            camera.take_picture()\n",
    "            rgba = camera.get_color_rgba()\n",
    "            plt.subplot(2, 5, steps//20+1)\n",
    "            plt.imshow(rgba[..., :3])\n",
    "        \n",
    "        if steps == 199:\n",
    "            print(\"robot qpos at steps 199: \", robot.get_qpos())\n",
    "\n",
    "        steps += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see that the robot arm fell down (due to gravity and other possible forces, like coriolis and centrifugal force). Let's find out how to keep the robot at a certain pose."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compensate passive forces"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a real robot, gravity compensation is done by an internal controller hardware. So it is usually desirable to skip this troublesome calculation of how to compensate gravity. SAPIEN provides `compute_passive_force` to compute desired forces or torques on joints to compensate passive forces:\n",
    "    \n",
    "    qf = robot.compute_passive_force(gravity=True, coriolis_and_centrifugal=True)\n",
    "    robot.set_qf(qf)\n",
    "\n",
    "In this example, we only consider gravity as well as coriolis and centrifugal force.\n",
    "\n",
    "Change the hyperparameter `balance_passive_force` at the top to `True` and rerun the notebook. You should observe that this time the robot can stay at the target pose. However, note that there is slight numeric differences between the initial `qpos` and the `qpos` at steps 200. In fact, due to numeric error, the robot will gradually deviate from the target pose. If you are running locally and visualizing with a viewer, you might even observe this deviation as the simulation continues.\n",
    "\n",
    "> Note: To avoid deviating from the target pose gradually, either we specify the damping (resistence proportional to velocity) of each joint in the URDF XML, or a controller can be used to compute desired extra forces or torques to keep the robot around the target pose. In the next tutorial, we will elaborate how to control the robot with a controller."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sapien",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52cc33f0f2a37b136af3b056cd1b16fcde515625a8da3d7221199fda542f2c7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
