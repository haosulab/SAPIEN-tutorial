{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/haosulab/SAPIEN-tutorial/blob/master/rendering/1_camera.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Some core features of SAPIEN are not available on Colab, including the interactive viewer and ray-tracing functionalities. You need to run SAPIEN locally for full features. You can also find the latest SAPIEN tutorial at [SAPIEN's documentation](https://sapien.ucsd.edu/docs/latest/index.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rendering with SAPIEN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAPIEN is integrated with the powerful `SapienRenderer`, which supports both rasterization and ray tracing. With `SapienRenderer`, you can generate photorealistic images or depth maps at an incredibly high speed. In this tutorial series, you will learn how to render with SAPIEN."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rendering Tutorial 1: Camera"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you will learn the following:\n",
    "\n",
    "- Create a camera `CameraEntity` and mount it to an actor\n",
    "- Off-screen rendering for RGB, depth, point cloud and segmentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-IXA-7qRy4ML"
   },
   "source": [
    "## Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ssNPcLKYDoDH"
   },
   "source": [
    "> Note: you need GPU runtime to run the notebook. If you are running on Colab, you might see a warning asking you to restart the runtime after running the following cell for the first time. In that case, restart the runtime as instructed and rerun the cell (Otherwise there might be some issues with imported packages). The warning should disappear after restarting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qIDL-wHKzKQ5"
   },
   "outputs": [],
   "source": [
    "%pip install sapien\n",
    "%pip install open3d\n",
    "\n",
    "import sapien.core as sapien\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import os\n",
    "from PIL import Image, ImageColor\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and mount a camera"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let’s set up the engine, renderer, scene, lighting, and load a URDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sapien.Engine()\n",
    "renderer = sapien.SapienRenderer()\n",
    "engine.set_renderer(renderer)\n",
    "\n",
    "scene = engine.create_scene()\n",
    "scene.set_timestep(1 / 100.0)\n",
    "\n",
    "if IN_COLAB:\n",
    "    !gdown -q 1phqv-pvgvYHmkJKI3KH8uHwxwf-HKA-m\n",
    "    !unzip -o -q 179.zip\n",
    "    assets_dir = \".\"\n",
    "else:\n",
    "    assets_dir = \"../assets\"\n",
    "\n",
    "urdf_path = os.path.join(assets_dir, \"179/mobility.urdf\")\n",
    "\n",
    "loader = scene.create_urdf_loader()\n",
    "loader.fix_root_link = True\n",
    "asset = loader.load_kinematic(urdf_path)\n",
    "\n",
    "\n",
    "scene.set_ambient_light([0.5, 0.5, 0.5])\n",
    "scene.add_directional_light([0, 1, -1], [0.5, 0.5, 0.5], shadow=True)\n",
    "scene.add_point_light([1, 2, 2], [1, 1, 1], shadow=True)\n",
    "scene.add_point_light([1, -2, 2], [1, 1, 1], shadow=True)\n",
    "scene.add_point_light([-1, 0, 1], [1, 1, 1], shadow=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the Vulkan-based renderer by calling `sapien.SapienRenderer(offscreen_only=...)`. If `offscreen_only=True`, the on-screen display is disabled. It works without a window server like x-server. You can forget about all the difficulties working with x-server and OpenGL!\n",
    "\n",
    "Next, you can create a camera as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "near, far = 0.1, 100\n",
    "width, height = 640, 480\n",
    "camera = scene.add_camera(\n",
    "    name=\"camera\",\n",
    "    width=width,\n",
    "    height=height,\n",
    "    fovy=np.deg2rad(35),\n",
    "    near=near,\n",
    "    far=far,\n",
    ")\n",
    "camera.set_pose(sapien.Pose(p=[1, 0, 0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This camera is now placed at coordinate [1, 0, 0] without rotation.\n",
    "\n",
    "An camera can also be mounted onto an `Actor` to keep a pose relative to the actor as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_mount_actor = scene.create_actor_builder().build_kinematic()\n",
    "camera.set_parent(parent=camera_mount_actor, keep_pose=False)\n",
    "\n",
    "# Compute the camera pose by specifying forward(x), left(y) and up(z)\n",
    "cam_pos = np.array([-2, -2, 3])\n",
    "forward = -cam_pos / np.linalg.norm(cam_pos)\n",
    "left = np.cross([0, 0, 1], forward)\n",
    "left = left / np.linalg.norm(left)\n",
    "up = np.cross(forward, left)\n",
    "mat44 = np.eye(4)\n",
    "mat44[:3, :3] = np.stack([forward, left, up], axis=1)\n",
    "mat44[:3, 3] = cam_pos\n",
    "camera_mount_actor.set_pose(sapien.Pose.from_transformation_matrix(mat44))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The camera is mounted on the the `camera_mount_actor` through `set_parent`. The pose of the camera relative to the mount is specified through `set_local_pose`.\n",
    "\n",
    "> Note: Calling `set_local_pose` without a parent sets the global pose of the camera. Callling `set_pose` with a parent results in an error, as it is ambiguous.\n",
    "\n",
    "The process of adding and mounting a camera can be achieved through the convenience function `add_mounted_camera`. The following cell will have the exact same effect as first `add_camera` and then `set_parent`.\n",
    "\n",
    "    near, far = 0.1, 100\n",
    "    width, height = 640, 480\n",
    "    camera_mount_actor = scene.create_actor_builder().build_kinematic()\n",
    "    camera = scene.add_mounted_camera(\n",
    "        name=\"camera\",\n",
    "        actor=camera_mount_actor,\n",
    "        pose=sapien.Pose(),  # relative to the mounted actor\n",
    "        width=width,\n",
    "        height=height,\n",
    "        fovy=np.deg2rad(35),\n",
    "        near=near,\n",
    "        far=far,\n",
    "    )\n",
    "\n",
    "If the mounted actor is kinematic (or static), the camera moves along with the actor when the actor of the actor is changed through set_pose. If the actor is dynamic, the camera moves along with it during dynamic simulation.\n",
    "\n",
    "> Note: Note that the axes conventions for SAPIEN follow the conventions for robotics, while they are different from those for many graphics softwares (like OpenGL and Blender). For a SAPIEN camera, the x-axis points forward, the y-axis left, and the z-axis upward.\n",
    ">\n",
    "> However, do note that the “position” texture (camera-space point cloud) obtained from the camera still follows the graphics convention (x-axis right, y-axis upward, z-axis backward). This maintains consistency of SAPIEN with most other graphics software. This will be further discussed below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render an RGB image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To render from a camera, you need to first update all object states to the renderer. Then, you should call `take_picture()` to start the rendering task on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene.step()  # make everything set\n",
    "scene.update_render()\n",
    "camera.take_picture()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can acquire the RGB image rendered by the camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgba = camera.get_float_texture('Color')  # [H, W, 4]\n",
    "# An alias is also provided\n",
    "# rgba = camera.get_color_rgba()  # [H, W, 4]\n",
    "rgba_img = (rgba * 255).clip(0, 255).astype(\"uint8\")\n",
    "Image.fromarray(rgba_img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate point cloud"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Point cloud is a common representation of 3D scenes. The following code showcases how to acquire the point cloud in SAPIEN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each pixel is (x, y, z, render_depth) in camera space (OpenGL/Blender)\n",
    "position = camera.get_float_texture('Position')  # [H, W, 4]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We acquire a “position” image with 4 channels. The first 3 channels represent the 3D position of each pixel in the OpenGL camera space, and the last channel stores the z-buffer value commonly used in rendering. When is value is 1, the position of this pixel is beyond the far plane of the camera frustum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenGL/Blender: y up and -z forward\n",
    "points_opengl = position[..., :3][position[..., 3] < 1]\n",
    "points_color = rgba[position[..., 3] < 1][..., :3]\n",
    "# Model matrix is the transformation from OpenGL camera space to SAPIEN world space\n",
    "# camera.get_model_matrix() must be called after scene.update_render()!\n",
    "model_matrix = camera.get_model_matrix()\n",
    "points_world = points_opengl @ model_matrix[:3, :3].T + model_matrix[:3, 3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the position is represented in the OpenGL camera space, where the negative z-axis points forward and the y-axis is upward. Thus, to acquire a point cloud in the SAPIEN world space (x forward and z up), we provide `get_model_matrix()`, which returns the transformation from the OpenGL camera space to the SAPIEN world space. Let's visualize the point cloud by Open3D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points_world)\n",
    "pcd.colors = o3d.utility.Vector3dVector(points_color)\n",
    "coord_frame = o3d.geometry.TriangleMesh.create_coordinate_frame()\n",
    "o3d.visualization.draw_plotly([pcd, coord_frame])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides, the depth map can be obtained as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = -position[..., 2]\n",
    "depth_image = (depth * 1000.0).astype(np.uint16)\n",
    "Image.fromarray(depth_image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point cloud and depth map are based on the *clean* depth obtained during the rendering process. In future tutorial, you will see that SAPIEN is also able to generate *realistic* depth that looks extremely similar to the depth computed by real-world depth sensors. Using realistic depth can greatly help closing the sim-to-real gap, which we will discuss in the future."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize segmentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAPIEN provides the interfaces to acquire object-level segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_labels = camera.get_uint32_texture('Segmentation')  # [H, W, 4]\n",
    "colormap = sorted(set(ImageColor.colormap.values()))\n",
    "color_palette = np.array([ImageColor.getrgb(color) for color in colormap],\n",
    "                            dtype=np.uint8)\n",
    "label0_image = seg_labels[..., 0].astype(np.uint8)  # mesh-level\n",
    "label1_image = seg_labels[..., 1].astype(np.uint8)  # actor-level\n",
    "# Or you can use aliases below\n",
    "# label0_image = camera.get_visual_segmentation()\n",
    "# label1_image = camera.get_actor_segmentation()\n",
    "label0_pil = Image.fromarray(color_palette[label0_image])\n",
    "print(\"Mesh-level segmentation\")\n",
    "display(label0_pil)\n",
    "print(\"Actor-level segmentation\")\n",
    "label1_pil = Image.fromarray(color_palette[label1_image])\n",
    "display(label1_pil)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
